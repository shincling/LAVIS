 # Copyright (c) 2022, salesforce.com, inc.
 # All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause

# Overall Accuracy is: 43.98
model:
  arch: blip2_t5
  model_type: pretrain_flant5xxl
  use_grad_checkpoint: False
  # pretrained: "/data2/shij/codes/BLIP2/LAVIS/lavis/output/BLIP2/Pretrain_stage2_t5/20230228122/checkpoint_1.pth" # T5 encoder 变成固定prompt
  # pretrained: "/data2/shij/codes/BLIP2/LAVIS/lavis/output/BLIP2/Pretrain_stage2_t5/20230301082/checkpoint_1.pth"# T5 encoder 变成固定prompt
  # pretrained: "/data2/shij/codes/BLIP2/LAVIS/lavis/output/BLIP2/Pretrain_stage2_t5/20230301093/checkpoint_49.pth" # T5 encoder 变成固定prompt
  # pretrained: "/data2/shij/codes/BLIP2/LAVIS/lavis/output/BLIP2/Pretrain_stage2_t5/20230301122/checkpoint_149.pth" # T5 encoder 变成固定prompt
  # pretrained: "/data2/shij/codes/BLIP2/LAVIS/lavis/output/BLIP2/Pretrain_stage2_t5/20230301150/checkpoint_99.pth" # T5 encoder 变成固定prompt

  # pretrained: "/data2/shij/codes/BLIP2/LAVIS/lavis/output/BLIP2/Pretrain_stage2_t5/20230302082/checkpoint_1.pth" # T5 encoder 变成固定prompt
  # pretrained: "/data2/shij/codes/BLIP2/LAVIS/lavis/output/BLIP2/Pretrain_stage2_t5/20230302120/checkpoint_0.pth" # T5 encoder 变成固定prompt
  # pretrained: "/data2/shij/codes/BLIP2/LAVIS/lavis/output/BLIP2/Pretrain_stage2_t5/20230302004/checkpoint_250.pth" # T5 encoder 变成固定prompt
  pretrained: "/data2/shij/codes/BLIP2/LAVIS/lavis/output/BLIP2/Pretrain_stage2_t5/20230315070/checkpoint_99.pth" # T5 encoder 变成固定prompt


datasets:
  gqa: # name of the dataset builder
    type: balanced_testdev
    vis_processor:
        eval:
          name: "blip_image_eval"
          image_size: 224
    text_processor:
        eval:
          name: "blip_question"
    # build_info:
    #     images:
    #         storage: "/export/share/datasets/vision/GQA/images/"

  # med_caption:
  #   vis_processor:
  #       train:
  #         name: "blip2_image_train"
  #         image_size: 224
  #       eval:
  #         name: "blip_image_eval"
  #         image_size: 224
  #   text_processor:
  #       train:
  #         name: "blip_caption"

run:
  task: gqa
  # optimization-specific
  batch_size_train: 16
  batch_size_eval: 8
  num_workers: 4

  # inference-specific
  max_len: 10
  min_len: 1
  num_beams: 5
  inference_method: "generate"
  prompt: "Question: {} Short answer:"

  seed: 42
  output_dir: "output/BLIP2/Med_qa_zeroshot/"

  evaluate: True
  test_splits: ["val"]

  # distribution-specific
  device: "cuda"
  world_size: 1
  dist_url: "env://"
  distributed: True
